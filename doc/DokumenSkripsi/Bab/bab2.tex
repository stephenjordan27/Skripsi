%versi 2 (8-10-2016)\chapter{Landasan Teori}
\chapter{Dasar Teori}
\label{chap:teori}

\section{Privasi}
\label{sec:privasi}
Privasi adalah suatu keadaan dimana kehidupan pribadi seseorang atau sekelompok orang terbebas dari pengawasan atau gangguan orang lain. Privasi juga dapat berarti kemampuan satu atau sekelompok individu untuk menutupi atau melindungi kehidupan dan urusan personalnya dari publik dengan mengontrol sumber-sumber informasi mengenai diri mereka. Untuk melakukan publikasi data dari satu perusahaan ke perusahaan lain, digunakan teknik anonimisasi data untuk melindungi dan menyamarkan atribut sensitif untuk setiap data.

\par \textit{Personally Identifiable Information} (PII) adalah standar yang digunakan untuk menentukan apakah informasi yang ada dapat melakukan identifikasi entitas individu secara lansung atau tidak langsung. PII menjelaskan bahwa identifikasi entitas secara langsung dapat dilakukan menggunakan atribut sensitif. Sedangkan identifikasi entitas secara tidak langsung dapat dilakukan menggunakan penggabungan beberapa atribut non-sensitif. PII adalah atribut  yang biasanya terjadi pelanggaran data dan pencurian identitas. Jika data perusahaan atau organisasi terungkap, maka sangat mungkin data pribadi seseorang akan terungkap. Informasi yang diketahui dapat dijual dan digunakan untuk melakukan pencurian identitas, menempatkan korban dalam risiko.
\\\\
Berikut adalah contoh informasi yang bersifat sensitif menurut standar PII:

\begin{itemize}
\item Identitas diri \\ 
Nama lengkap, tempat tanggal lahir, alamat rumah, alamat email.
\item Nomor identitas diri \\
NIK, nomor passport, nomor SIM, nomor wajib pajak, nomor rekening, nomor telepon, dan nomor kartu kredit.
\item Karakteristik pribadi  \\
Foto diri, sidik jari, dan tulisan tangan.
\item Data biometrik \\
Pemindaian retina, jenis suara, dan geometri wajah.
\item Aset informasi lainnya \\
IP Address dan Media Access Control (MAC). 
\end{itemize}

\noindent Berikut adalah contoh informasi yang bersifat non-sensitif menurut standar PII:
\begin{itemize}
\item Rekaman medis
\item Riwayat pendidikan
\item Riwayat pekerjaan 
\item Informasi finasial
\item Letak geografis
\end{itemize}

\section{Data Mining}
Data yang dikumpulkan bertambah banyak, sehingga perlu adanya cara untuk melakukan proses ekstraksi informasi pada sekumpulan data yang sangat banyak. Menurut Gartner, \textit{data mining} adalah proses menemukan korelasi, pola, dan tren baru yang bermakna dengan menyaring sejumlah besar data yang disimpan menggunakan teknologi pengenalan pola serta teknik statistik dan matematika. \textit{Data mining} merupakan bagian dari \textit{Knowledge Discovery in Databases} (KDD). KDD adalah proses transformasi sekumpulan data yang disimpan pada basis data menjadi informasi yang berguna.\\

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{datamining1}
	\caption{Tahapan pada KDD}
	\label{fig:datamining1}
\end{figure}

\noindent Berikut ini adalah penjelasan tahapan pada KDD pada Gambar \ref{fig:datamining1} sebagai berikut:

\begin{enumerate}
\item \textit{Selection}: proses mengambil data yang relevan terhadap analisis.
\item \textit{Preprocessing}: proses pembersihan data dari data yang tidak konsisten dan integrasi data saat penggabungan data.
\item \textit{Transformation}: proses manipulasi data menggunakan konsep agregasi, generalisasi, normalisasi, dan reduksi untuk kebutuhan analisis.
\item \textit{Data mining}: proses ekstraksi informasi menggunakan metode pengenalan pola seperti klasifikasi, pengelompokan/\textit{clustering}.
\item \textit{Interpretation/evaluation}: proses interpretasi hasil pengolahan data menjadi sebuah grafik yang dapat dimengerti.
\end{enumerate}


\noindent Berikut adalah beberapa jenis tipe data terkait teknik data mining:

\begin{itemize}

\item \textit{Binary}: tipe data alphabet/numerik yang hanya memiliki 2 kemungkinan nilai. \\ Contoh: nilai true/false dan 0/1.

\item \textit{Nominal}: tipe data alphabet/numerik yang memiliki lebih dari 2 kemungkinan nilai.\\ Contoh: warna  kuning, hijau, hitam, merah.

\end{itemize}

\noindent Tujuan dari penggunaan teknik \textit{data mining} adalah sebagai berikut:

\begin{itemize}

\item Prediksi: proses menggunakan nilai dari beberapa atribut yang sudah ada untuk memprediksi nilai atribut di masa yang akan datang. Contoh: klasifikasi.

\item Deskripsi: proses menemukan pola yang dapat merepresentasikan kelompok dari sebuah data. Contoh: pengelompokan/\textit{clustering}.

\end{itemize}

\subsection{Klasifikasi} 
Klasifikasi adalah proses menemukan model (atau fungsi) yang cocok untuk mendeskripsikan dan membedakan sebuah kelas data dengan kelas data lain. Dalam pembelajaran mesin, klasifikasi sering dianggap sebagai contoh dari metode pembelajaran yang diawasi, yaitu menyimpulkan fungsi dari data pelatihan berlabel.\\

\noindent Berikut adalah tahapan klasifikasi secara umum:
\begin{enumerate}

\item
Pelatihan: proses konstruksi model klasifikasi menggunakan algoritma tertentu. Algoritma digunakan untuk membuat model belajar menggunakan set pelatihan data yang tersedia. Model dilatih untuk menghasilkan prediksi yang akurat.

\item
Klasifikasi: model yang digunakan untuk memprediksi label kelas dan menguji model yang dibangun pada data uji dan karenanya memperkirakan akurasi aturan klasifikasi.
\end{enumerate}
\vspace{0.3cm}
\noindent Berikut adalah kategori pemodelan klasifikasi:
\begin{itemize}

\item 
\textit{Discriminative}: pemodelan paling mendasar untuk menentukan satu kelas untuk setiap baris data. Pemodelan ini bergantung pada data yang diamati dan sangat bergantung pada kualitas data daripada distribusi data.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{klasifikasi1}
	\caption{Contoh Logistic Regression}
	\label{fig:klasifikasi1}
\end{figure}

Contoh: \textit{Logistic Regression}\\
Gambar \ref{fig:klasifikasi1} adalah penerimaan siswa pada sebuah Universitas, untuk mempertimbangkan \textit{test score} dan \textit{grades} terhadap keputusan seorang siswa diterima/tidak diterima.

\item 
\textit{Generative}: pemodelan ini memodelkan distribusi kelas individu dan mencoba mempelajari model yang menghasilkan data dengan memperkirakan asumsi dan distribusi model. Digunakan untuk memprediksi nilai data yang belum diketahui. \\\\
Contoh: \textit{Naive Bayes} \\
Mendeteksi email spam dengan melihat data sebelumnya. Misalkan dari 100 email yang ada dibagi menjadi kategori Kelas A: 25\% (Email spam) dan Kelas B: 75\% (Email Non-Spam). Ingin diperiksa apakah email berisi  spam atau bukan. Pada Kelas A, 20 dari 25 email adalah spam dan sisanya bukan spam. Pada Kelas B, 70 dari 75 email bukan spam dan sisanya adalah spam. Probabilitas email yang berisi spam termasuk pemodelan \textit{naive bayes}. \\
\end{itemize}

\noindent Berikut adalah contoh pemodelan yang umum digunakan:
\begin{itemize}
\item \textit{Decision Trees}
\item \textit{Naive Bayes}
\item \textit{Neural Networks}
\item \textit{K-Nearest Neighbour}
\item \textit{Linear Regression}
\end{itemize}

\subsection{Naive bayes}
\label{sec:naive_bayes}
\par \textit{Naive Bayes} menerapkan klasifikasi dengan menggunakan metode probabilitas dan statistik. Pemodelan ini mencari nilai probabilitas tertinggi pada masing-masing kelas menggunakan teorema \textit{Bayes}. Kelas dengan probabilitas tertinggi akan dipilih sebagai hasil akhir. \textit{Naive Bayes} mudah untuk dibangun dan memiliki komputasi yang lebih cepat daripada model klasifikasi lainnya.\\

\noindent Teorema \textit{Bayes} menemukan probabilitas suatu peristiwa terjadi mengingat probabilitas peristiwa lain yang telah terjadi. Teorema \textit{Bayes} dinyatakan secara matematis melalui persamaan berikut:

\begin{equation}
P(H|D) = \frac{P(D|H) \cdot P(H)}{P(D)}
\end{equation}

\noindent
Dari perhitungan probabilitas teorema Bayes, akan dicari kelas dengan probabilitas maksimum. Probabilitas maksimum dapat dinyatakan secara matematis melalui persamaan berikut:

\begin{align}
MAP(H) = max(P(H|D))
\end{align}

\noindent Keterangan:
\begin{itemize}
\item P(H|D) adalah probabilitas posterior apabila diberika hipotesis H dan diketahui data D. 
\item P(D|H) adalah probabilitas posterior data D jika hipotesis h adalah benar.
\item P(H) adalah probabilitas hipotesis h adalah benar 
\item P(D) adalah probabilitas data.
\end{itemize}

\vspace{0.3cm}

\noindent Gambar \ref{fig:naive_bayes1} diberikan untuk menggambarkan kondisi cuaca saat bermain golf. Masing-masing data dikategorikan berdasarkan nilai atribut \textit{PlayGolf}, yaitu cocok (\textit{Yes}) atau tidak cocok (\textit{No}). 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{naive_bayes1}
	\caption{Dataset Kondisi Cuaca Bermain Golf}
	\label{fig:naive_bayes1}
\end{figure}

\noindent Berikut adalah pengelompokan nilai berdasarkan dataset yang telah diberikan:

\begin{itemize}

\item 
Vektor fitur\\
Vektor fitur adalah vektor yang mewakili nilai fitur untuk setiap baris dataset. Vektor fitur dalam dataset ini tersusun dari nilai atribut \textit{Outlook, Temperature, Humidity, dan Windy}.

\item
Vektor respon\\
Vektor respon adalah nilai prediksi kelas untuk setiap vektor fitur. Vektor Respon dalam dataset ini diwakili oleh nilai atribut \textit{PlayGolf}.

\end{itemize}
\vspace{0.5cm}
\noindent Secara singkat, langkah kerja algoritma \textit{Naive Bayes} dapat dijelaskan sebagai berikut:

\begin{enumerate}
\item Merepresentasikan teorema Bayes terhadap vektor fitur.\\
Berdasarkan dataset, teorema Bayes dapat diubah seperti berikut:

\begin{equation}
P(y|X) = \frac{P(X|y) \cdot P(y)}{P(X)}
\end{equation}

Di mana y adalah variabel kelas dan X adalah vektor fitur (dengan ukuran n), dinyatakan melalui persamaan berikut:

\begin{equation}
X = (x_1, x_2, x_3, \ldots, x_n)
\end{equation}

Contoh: X = (Rainy, Hot, High, False), y = No
\\\\
Diasumsikan teorema \textit{Bayes} saling independen terhadap fitur-fiturnya. Berikut adalah persamaan teorema \textit{Bayes} baru, jika memakai lebih dari satu nilai atribut:

\begin{equation}
P(y|x_1,\ldots,x_n) = \frac{P(x_1|y) P(x_2|y) \ldots P(x_n|y) P(y)}{P(x_1) P(x_2) \ldots P(x_n)}
\end{equation}


\item Gambar \ref{fig:naive_bayes2} adalah contoh menghitung probabilitas masing-masing atribut.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{naive_bayes2}
	\caption{Menghitung Probabilitas}
	\label{fig:naive_bayes2}
\end{figure}

Contoh: menghitung $P(No)$ untuk nilai \textit{Sunny} pada atribut \textit{Outlook}
\begin{equation}
P(No) = \frac{frekuensi(Sunny \cap No)}{frekuensi(No)}
\end{equation}

Contoh: menghitung $P(Yes)$ untuk nilai \textit{Sunny} pada atribut \textit{Outlook}
\begin{equation}
P(Yes) = \frac{frekuensi(Sunny \cap Yes)}{frekuensi(Yes)}
\end{equation}

\item Menghitung probabilitas bersyarat jika diketahui nilai dari data baru. \\\\
Contoh: \textit{today = (Sunny, Hot, Normal, False)}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.73]{naive_bayes3}
	\label{fig:naive_bayes3}
\end{figure}

\begin{equation}
P(Yes|today) = \frac{3}{5} \cdot \frac{2}{5} \cdot \frac{1}{5} \cdot \frac{2}{5} \cdot \frac{5}{14} = 0.0068
\end{equation}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.73]{naive_bayes4}
	\label{fig:naive_bayes4}
\end{figure}

\begin{equation}
P(No|today) = \frac{2}{9} \cdot \frac{2}{9} \cdot \frac{6}{9} \cdot \frac{6}{9} \cdot \frac{9}{14} = 0.0068
\end{equation}

\item Melakukan normalisasi terhadap probabilitas besyarat.\\

Setelah probabilitas bersyarat dinormalisasi, akan menjadi seperti berikut:

\begin{equation}
P(Yes|today) = \frac{0.0141}{0.0141 + 0.0068} = 0.67
\end{equation}

\begin{equation}
P(No|today) = \frac{0.0068}{0.0141 + 0.0068} = 0.33
\end{equation}

Sehingga memiliki probabilitas total seperti berikut:

\begin{equation}
P(Yes|today) + P(No|today) = 1
\end{equation}

\item Mencari probabilitas tertinggi.\\

Berdasarkan pernyataan berikut:
\begin{equation}
P(Yes|today) > P(No|today)
\end{equation}

\noindent Dapat disimpulkan bahwa, jika diberikan data dengan nilai \textit{(Sunny, Hot, Normal, False)} klasifikasi yang tepat untuk atribut \textit{PlayGolf} adalah \textit{Yes}.

\end{enumerate}

\subsection{Pengelompokan/Clustering} 
\textit{Clustering} adalah salah satu teknik analisis data yang paling umum digunakan untuk mendapatkan kemiripan antar data. \textit{Clustering} dapat didefinisikan sebagai sebuah tugas untuk mengidentifikasi subkelompok dalam data sedemikian rupa sehingga titik data dalam subkelompok/\textit{cluster} yang sama sangat mirip sedangkan titik data dalam kelompok berbeda sangat berbeda. Contoh pemodelan \textit{clustering} adalah \textit{K-Means}.

\subsection{K-Means} 
\label{sec:k_means}
\textit Algoritma {k-means} adalah algoritma pembelajaran mesin \textit{unsupervised learning} untuk menentukan objek tersebut benar-benar milik kelompok data tertentu. \textit{Unsupervised learning} artinya tidak ada label yang ditentukan dalam data. Gagasan utama \textit{k-means} adalah menetapkan setiap data ke dalam cluster dengan mean terdekat (centroid). Mencari titik terdekat dilakukan dengan cara menghitung distance antara dua data menggunakan Euclidean distance, lalu membandingkat titik yang memiliki jarang paling dekat dengan titik lainnya. \\

\noindent Berikut adalah persamaan untuk menghitung \textit{Euclidean distance}:
\begin{equation}
EuclidDist(p_i,C_i) = \sqrt[]{(p_1-C_1)^2+(p_2-C_2)^2+\ldots +(p_n-C_n)^2}
\end{equation}
\vspace{0.2cm}

\noindent Gambar \ref{fig:kmeans_1} adalah skor A dan B untuk masing-masing individu:

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{kmeans_1}
	\caption{Contoh Dataset K-Means}
	\label{fig:kmeans_1}
\end{figure}

\noindent Secara singkat, langkah kerja algoritma \textit{k-means} dapat dijelaskan sebagai berikut:
\begin{enumerate}

\item Gambar \ref{fig:kmeans_2} adalah hasil pengelompokan awal untuk k = 2. Untuk menentukan titik centroid awal, akan dicari nilai A dan B terjauh dengan data lainnya menggunakan Euclidean distance.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{kmeans_2}
	\caption{Hasil Pengelompokan Awal}
	\label{fig:kmeans_2}
\end{figure}

\item Data yang tersisa akan diperiksa secara berurutan dan dialokasikan pada cluster yang paling dekat dengan \textit{centroid} awal menggunakan \textit{Euclidean distance}. Gambar \ref{fig:kmeans_3} menunjukan vektor rata-rata (centroid) akan dihitung ulang setiap kali anggota baru ditambahkan.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{kmeans_3}
	\caption{Mencari Centroid Kelompok}
	\label{fig:kmeans_3}
\end{figure}

\item Menentukan titik \textit{centroid} baru pada \textit{cluster} yang baru terbentuk dari tahap sebelumnya.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{kmeans_4}
	\caption{Hasil Pengelompokan Baru}
	\label{fig:kmeans_4}
\end{figure}

\item Belum bisa dipastikan bahwa setiap individu telah dialokasikan pada cluster yang tepat. Oleh karena itu, perlu membandingkan \textit{distance} masing-masing data dengan \textit{centroid} baru pada masing-masing kelompok. Gambar \ref{fig:kmeans_5} adalah tabel hasil perbandingan \textit{distance} yang dihitung menggunakan rumus \textit{Euclidian distance}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{kmeans_5}
	\caption{Euclidean Distance Cluster 1, Cluster 2}
	\label{fig:kmeans_5}
\end{figure}

\item Dapat disimpulkan bahwa, hanya individu 3 yang jaraknya lebih dekat dengan \textit{centroid Cluster 2} dari pada \textit{centroid Cluster 1}. Dengan kata lain, distance masing-masing individu ke centroid kelompoknya sendiri harus lebih kecil daripada rata-rata kelompok lain. Dengan demikian, individu 3 harus dialokasikan ke \textit{Cluster 2}. Gambar \ref{fig:kmeans_6} adalah hasil pengelompokan akhir yang dihasilkan oleh pemodelan \textit{k-means}.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{kmeans_6}
	\caption{Hasil Pengelompokan Akhir}
	\label{fig:kmeans_6}
\end{figure}



\end{enumerate}

\section{Privacy-Preserving Data Publishing} 
\textit{Privacy Preserving Data Publishing} (PPDP) adalah adalah pendekatan untuk mempublikasikan data praktis yang berguna tanpa melanggar privasi individu. PPDP berfokus pada data anonimisasi yang berusaha menyembunyikan identitas pemilik data.
\\

\noindent Berikut adalah penjelasan tahapan pada PPDP:

\begin{enumerate}

\item Pengumpulan data \\
Dalam fase ini, data asli dari pemegang rekaman adalah diambil oleh penerbit data.

\item Penerbitan data \\
Dalam fase ini, data diambil oleh pemegang catatan di tahap pengumpulan data, dirilis ke penerima data untuk keperluan analisis dan data mining

\end{enumerate}

\noindent Berikut adalah kategori dari penerbit data:
\begin{enumerate}

\item Penerbit Data Tepercaya \\
Pemilik data mengetahui bahwa penerbit data dapat diandalkan dan mereka bersedia memberikan informasi pribadi mereka untuk analisis.

\item Penerbit data yang tidak tepercaya\\
Pemilik data mengetahui bahwa penerbit data mungkin mencari cara untuk mendapatkan informasi rahasia dari pemilik data.

\end{enumerate}

\noindent Privacy-preserving data publishing dapat dianalogikan sebagai:
\begin{enumerate}

\item Pengguna data\\
Pihak yang ingin memanfaatkan data untuk kebutuhan analisis data.

\item Musuh\\
Pihak yang ingin  memperoleh informasi pribadi milik orang lain.

\item Penerbit data\\
Pihak tang mengumpulkan data dan ingin merilis data dengan cara yang memuaskan
kebutuhan data pengguna, tetapi juga ingi  mencegah musuh memperoleh informasi pribadi tentang data individu.

\item Individu \\
Pihak pemilik data, yang datanya dikumpulkan oleh penerbit data. Dalam beberapa kasus, individu setuju dengan kebijakan privasi penerbit data dengan mempercayai penerbit data dan memberikan penerbit data semua informasi yang diminta. 

\end{enumerate}

\noindent PPDP menyebabkan pertukaran mendasar antara privasi dan utilitas. Contoh kasus pertama adalah penerbit data sangat mungkin untuk menyamarkan seluruh nilai data saat merilis data, sehingga privasi dapat dilindungi dengan sempurna. Namun, tidak ada seorang pun yang dapat menggunakan data tersebut, sehingga data tersebut menjadi tidak berguna. Contoh kasus lainnya, penerbit data dapat menerbitkan kumpulan data tanpa modifikasi apa pun sehingga utilitas data dapat dimaksimalkan. Namun, perlindungan privasi tidak terjamin saat perilisan data. \\

\noindent Untuk merilis data yang informatif dan aman diperlukan ketentuan berikut:

\begin{itemize}

\item Mekanisme anonimisasi\\
Diberikan kumpulan data seperti pada Gambar \ref{fig:ppdp1}, mekanisme anonimisasi bertujuan untuk membersihkan set data dengan membuat data menjadi kurang akurat. Gambar \ref{fig:ppdp1} dan \ref{fig:ppdp1} adalah contoh hasil anonimisasi yang kurang baik dan yang baik.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{ppdp1}
	\caption{Kumpulan Data}
	\label{fig:ppdp1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{ppdp2}
	\caption{Hasil Anonimisasi 1}
	\label{fig:ppdp2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{ppdp3}
	\caption{Hasil Anonimisasi 2}
	\label{fig:ppdp3}
\end{figure} 

\item Kriteria privasi\\
Diberikan kandidat rilis, privasi Kriteria mendefinisikan apakah kandidat rilis aman untuk lepaskan atau tidak. Contohnya adalah k-anonymity

\item Metrik utilitas\\
Diberikan kandidat data yang akan dirilis, metrik utilitas mengkuantifikasi utilitas data dari kandidat data yang dipublikasi. Contohnya adalah nilai "man" dan "women" diganti dengan nilai yang lebih umum "person".

\end{itemize}




\section{Model Serangan pada Privacy Preserving Data Publishing} 
Menurut Dalenius (1977), perlindungan privasi tidak memberikan kesempatan bagi orang lain untuk mendapatkan informasi sensitif individu meskipun orang lain mengetahui informasi umum yang berhubungan dengan informasi sensitif individu tersebut. Secara umum, orang lain dapat menemukan sebuah cara untuk memetakan sebuah data ke dalam tabel yang telah dianonimisasi ketika data tersebut telah dipublikasi. Serangan ini dikenal dengan istilah \textit{linkage attack}. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{record_linkage1}
	\caption{Tabel Pasien}
	\label{fig:record_linkage1}
\end{figure} 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{record_linkage2}
	\caption{3-Anonymous Table}
	\label{fig:record_linkage2}
\end{figure} 

\subsection{\textit{Record Linkage}}
\textit{Record Linkage} mengacu pada pemetaan beberapa catatan kepada korban yang ditargetkan di tabel yang dirilis untuk umum berdasarkan kuasi-identifier dari korban. Jika korban pengidentifikasi kuasi cocok dengan catatan dalam tabel yang dirilis kemudian musuh wajah kurang tidak. kemungkinan untuk catatan yang ditargetkan. Dengan beberapa informasi tambahan. 

\par Beberapa data pasien dipetakan ke dalam tabel pada Gambar \ref{fig:record_linkage1} berdasarkan nilai quasi-identifiers yang sama. Pemetaan data dilakukan terlebih dahulu, agar proses ananonimisai dapat dilakukan dengan mudah pada nilai quasi-idetifier yang sama. Gambar \ref{fig:record_linkage2} adalah hasil anonimisasi pada tabel pasien dengan nilai k = 3. Nilai k menentukan kelompok data yang terbentuk berdasarkan pemodelan k-anonymity. Perlu diperhatikan pada Gambar \ref{fig:record_linkage1}, k-anonymity melakukan anonimisasi data pada nilai quasi -identifier yang unik contohnya jenis kelamin (laki-laki), usia (38 tahun), profesi (pengacara), dan penyakit (HIV) menjadi nilai yang lebih umum.

\par Untuk menghindari jenis serangan seperti itu dengan catatan hubungan, teknik baru diusulkan oleh Sweeney, Samrati [9] dalam model ini untuk setiap set semua pengidentifikasi semu yang dimiliki nilai yang sama dalam tabel harus memiliki minimal k jumlah catatan. Manfaatnya Modelnya adalah bahwa ada tupel (k-1) lainnya yang dipetakan ke pengenal kuasi yang sama atur dengan probabilitas serangan 1 / k. Seperti yang ditunjukkan pada tabel 1 untuk pengidentifikasi kuasi (pekerjaan, kelahiran, kode pos). Jika tabel adalah k anonim dengan seperangkat quasi-identifier Q, maka harus memenuhi k-anonymity sehubungan dengan semua subset Q.

\subsection{\textit{Attribute Linkage}} 
Pada \textit{Attribute Linkage}, penyerang mendapatkan beberapa informasi tentang atribut sensitifnya dari tabel dirilis, meskipun penyerang tidak dapat menghubungkan korban dengan siapa pun catatan yang diterbitkan secara individu. Dari tabel 1.6, penyerang dapat menemukan semua itu wanita berusia 30 tahun yang profesinya menari menderita HIV.so {Dance, Female, 30} percaya diri 100 persen HIV dengan informasi ini ia menemukan bahwa Emily menderita dari HIV. 

\par Model l-diversity mencegah serangan atribut linkage. Kondisi yang diperlukan adalah setiap ekuivalensi yang dilepaskan tabel harus memiliki setidaknya l nilai yang berbeda. Konsep dasarnya adalah untuk menghindari hubungan atribut jika ada perbedaan yang unik antar nilai sensitif untuk mencegah hubungan atribut. Namun, serangan probabilistik tidak bisa dihindari, contohnya penyakit "flu" adalah penyakit yang sangat umum dibandingkan HIV. 

\section{Anonimisasi}
\label{sec:anonimisasi}
Tabel seperti biasa tidak memberikan perlindungan privasi. Untuk menjaga privasi masing-masing data, perlu dilakukan metode tertentu. Anonimisasi adalah teknik  perlindungan informasi bagi pemilik data dengan menyembunyikan identitas eksplisit  pemilik data terkait publikasi data. Pendekatan anonimisasi terdiri dari generalisasi dan supresi.

\subsection{Generalisasi}
Generalisasi memodifikasi nilai asli yang paling spesifik menjadi nilai yang lebih umum dari deskripsi spesifik, misalnya tanggal lahir digeneralisasi menjadi tahun dengan menyembunyikan nilai bulan dan tanggal. Generalisasi menggunakan tingkatan hierarki pohon. \\

\noindent Jenis generalisasi dapat didefinisikan sebagai berikut:

\begin{itemize}
\item Generalisasi Subtree\\
Dalam skema generalisasi subtree, node-node selain node daun \textit{child} dapat dilakukan generalisasi atau tidak perlu dilakukan generalisasi. 

\item Generalisasi Sibling\\
Skema generalisasi ini sama dengan generalisasi subtree, akan tetapi beberapa sibling pada node \textit{parent} yang sama tidak perlu dilakukan generalisasi. 

\item Generalisasi Sel\\
Skema generalisasi ini bertujuan untuk melakukan generalisasi nilai dalam sebuah baris data, sehingga nilai yang spesifik harus digeneralisasi pada semua data.

\end{itemize}

\subsection{Supresi}
Supresi mirip dengan generalisasi tetapi nilai-nilai quasi-identifier benar-benar dihilangkan. Misalnya pada atribut jenis kelamin nilai "man" dan "woman diubah menjadi "any" atau beberapa nilai tersebut akan dihilangkan sama sekali dari tabel dataset. \\

\noindent Jenis supresi dapat didefinisikan sebagai berikut:
\begin{itemize}
\item Level Data\\
Ketika sebuah data dihilangkan pada tabel .
\item Level Nilai\\
Ketika beberapa nilai tertentu dihilangkan pada tabel.
\item Level Sel\\
Ketika beberapa data dihilangkan pada tabel.
\end{itemize}

\section{Hierarchy Based Generalization} 
\label{theory:hierarchy_generalization}
\textit{Hierarchy-based generalization} adalah tahapan anonimisasi setelah data yang memiliki \textit{quasi-identifier} yang sama dikelompokan ke dalam kelas yang sama. \textit{Hierarchy-based generalization} menggunakan konsep generalisasi dan supresi dalam melakukan anonimisasi. \textit{Hierarchy-based generalization} termasuk metode \textit{full-domain generalization}. \textit{Full-domain generalization} diusulkan oleh Samarati dan Sweeney untuk memetakan seluruh domain untuk masing-masing atribut \textit{quasi-identifier} pada tabel ke domain yang lebih umum berdasarkan kategori tertentu.

\par \textit{Full-domain generalization} menetapkan bahwa metode generalisasi dapat dilakukan jika \textit{quasi-identifier} telah ditentukan sebelumnya. Sebagai contoh QI = $\{A_1, \ldots , A_n\}$, dimana A adalah atribut dari tabel dataset. \textit{Full-domain generalization} digunakan oleh \textit{k-anonymity} untuk menentukan generalisasi dan supresi dari sebuah nilai. Terdapat dua jenis hierarki pada \textit{Full-domain generalization} yaitu Domain Generalization Hierarchy (DGH) dan \textit{Value Generalization Hierarchy (VGH)}. Jenis hierarki yang paling umum digunakan adalah DGH. Tidak terdapat aturan khusus untuk memodelkan hierarki DGH. Semua nilai atribut dalam tabel harus bisa digeneralisasi menggunakan DGH. DGH adalah konsep sederhana dari penggantian nilai berdasarkan nilai yang kurang spesifik menjadi nilai lebih umum terhadap nilai aslinya. 

Pada Gambar \ref{fig:DGH1}, nilai ZIP \{02138, 02139\} dapat digeneralisasi menjadi \{0213*\} dengan menghilangkan digit paling kanan. Nilai yang telah digeneralisasi akan memiliki lingkup nilai yang lebih besar. Sebagai contoh, nilai ZIP adalah \{02138, 02139\} berada pada domain dasar yaitu $Z_0$. Untuk mencapai perlindungan data pada \textit{k-anonymity}, maka kode ZIP yang sebelumnya unik harus diubah menjadi bentuk umum. Sebuah nilai dapat digeneralisasi jika memiliki domain yang lebih umum. Sedangkan domain yang kurang spesifik digunakan untuk mendeskripsikan garis besar nilai ZIP. Sebagai contoh dari domain kurang spesifik adalah $Z_1$, di mana digit terakhir diganti dengan simbol (*). Berikut adalah contoh pemetaan dari Z0 ke Z1 seperti berikut $02139 \rightarrow 0213*$.

Representasi generalisasi diperluas agar metode supresi dapat diterapkan dengan cara membuat hirarki generalisasi untuk elemen dengan nilai maksimal yang baru berada di atas elemen dengan nilai maksimal yang lama. Elemen maksimal baru adalah nilai supresi atribut. Ketinggian setiap hierarki generalisasi akan bertambah seiring munculnya elemen maksimal yang baru. Setelah elemen mencapai nilai maksimal yang dapat digeneralisasi maka tidak akan ada lagi perubahan yang diperlukan untuk memasukkan nilai supresi (*). Gambar \ref{fig:DGH1} dan Gambar \ref{fig:DGH2} adalah contoh hierarki generalisasi dari domain umum yaitu (*****). 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{DGH1}
	\caption{DGH dan VGH pada atribut ZIP}
	\label{fig:DGH1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{DGH2}
	\caption{DGH dan VGH pada atribut Race}
	\label{fig:DGH2}
\end{figure}

\section{K-Anonymity}
\textit{K-anonymity} merupakan model yang paling efektif untuk melindungi privasi saat melakukan publikasi data. \textit{K-anonymity} adalah contoh pemodelan dari keamanan informasi yang bertujuan agar sebuah data tidak dapat dibedakan setidaknya dengan k-1 data lainnya. Dengan kata lain, penyerang tidak dapat mengidentifikasi identitas dari satu data karena k-1 data yang lain memiliki sifat yang sama. Dalam pemodelan k-anonymity, nilai k dapat digunakan sebagai tingkat keamanan privasi. Semakin tinggi nilai k, semakin sulit untuk mengindentifikasi sebuah data. Secara teori, probabilitas identifikasi sebuah data adalah 1 / k. Namun, meningkatkan nilai k juga berpengaruh terhadap nilai informasi yang diperoleh dari sekumpulan data.

\par Penelitian menunjukkan bahwa sebagian besar pemodelan \textit{k-anonymity} menggunakan metode generalisasi dan supresi. Pendekatan tersebut menderita kehilangan informasi yang signifikan karena mereka sangat bergantung terhadap hubungan relasi antar atribut. Oleh karena itu, hasil anonimisasi menghasilkan nilai kehilangan informasi yang cukup tinggi. Selain itu, algoritma anonimisasi yang ada hanya berfokus pada perlindungan informasi pribadi dan mengabaikan utilitas data yang sebenarnya. Akibatnya, nilai utilitas pada data yang telah dianonimisasi memiliki bernilai rendah. Beberapa algoritma dicoba pada pemodelan \textit{k-anonymity}.

\par Algoritma \textit{k-means clustering} akan melakukan beberapa iterasi sampai \textit{centroid} dari semua data tidak lagi berubah atau perubahannya kecil. Algoritma \textit{k-means clustering} tidak mampu untuk menyelesaikan masalah pada atribut yang bernilai kategorikal. Kelebihan dari algoritma \textit{k-means clustering} adalah memiliki hasil pengelompokan yang sudah baik. Kekurangan dari algoritma \textit{k-means clustering} adalah pemilihan centroid awal k-means secara acak, sehingga setelah digeneralisasi hasil pengelompokannya mengakibatkan hilangnya informasi yang besar.

\par Algoritma \textit{k-member} dapat melakukan generalisasi atribut kategorikal dengan memperoleh kualitas informasi yang lebih baik daripada algoritma \textit{k-means clustering}. Namun algoritma \textit{k-member} masih memiliki masalah ketika melakukan pengelompokan data. Kekurangan dari algoritma \textit{k-member} adalah hanya mempertimbangkan pengelompokan data pada baris yang terakhir tanpa memperhatikan pengelompokan yang dihasilkan pada proses sebelumnya sehingga menyebabkan distribusi kelompok data pada beberapa bagian menjadi kurang tepat. 

\par Untuk menghindari kekurangan pada algoritma \textit{k-means} dan algoritma \textit{k-member} maka kedua pendekatan ini digabung menjadi algoritma baru yaitu algoritma \textit{Greedy k-member clustering}. Algoritma \textit{Greedy k-member clustering} mendapatkan hasil pengelompokan yang lebih tepat dan memiliki nilai informasi yang lebih baik meskipun dilakukan generalisasi. Hasil akhir dari algoritma \textit{Greedy k-member clustering} adalah data-data yang sejenis telah dikelompokan pada kelompok data yang sama. Untuk melakukan anonimisasi, digunakan konsep \textit{Hierarchy Based Generalization}. \textit{K-anonymity} menyamarkan data pada nilai quasi-identifier yang sama. 

\vspace{0.5cm}
\noindent Berikut adalah atribut dari pemodelan \textit{k-anonymity}, yaitu:
 
\begin{itemize}
\item \textit{Identifier} (ID) adalah atribut yang unik dan secara langsung dapat mengidentifikasi seseorang seperti nama, nomor ID, dan nomor ponsel.
\item \textit{Quasi-identifier} (QID) adalah kombinasi atribut yang mungkin terjadi untuk mengidentifikasi individu berdasarkan penggabungan informasi lain dari luar. Seluruh atribut data terkecuali atribut identifier dapat dianggap sebagai atribut quasi-identifier.
\item \textit{Sensitive Attribute} (SA) adalah atribut yang menyangkut informasi  sensitif seseorang, biasanya nilai atribut ini akan dirahasiakan saat distribusi data.
\item \textit{Non-sensitive Attribute} (NSA) adalah atribut yang tidak menyangkut informasi sensitif seseorang, biasanya nilai atribut ini langsung ditampilkan saat distribusi data.
\end{itemize}


\section{Greedy K-Member Clustering}
\label{sec:greedyclustering}
Penelitian menunjukkan bahwa sebagian besar metode \textit{k-anonymity} didasarkan pada generalisasi dan teknik penekanan sehingga menderita dari kehilangan informasi yang signifikan. Masalah pengelompokan dapat meminimalkan kehilangan informasi melalui algoritma \textit{k-member clustering}. Akan tetapi algoritma \textit{k-member clustering} berpotensi memiliki kompleksitas yang eksponensial. Untuk menurunkan kompleksitas tersebut, maka permasalahan algoritma \textit{k-member clustering} dapat didefinisikan sebagai permasalahan algoritma \textit{Greedy}. Algoritma \textit{Greedy k-Member clustering} bertujuan untuk membagi seluruh tuple pada dataset ke masing-masing \textit{cluster} dengan kompleksitas yang lebih baik dan mendukung informasi yang lebih banyak dibandingkan algoritma \textit{clustering} yang lain.

\vspace{0.5cm}
\begin{theorem}
Masalah pengambilan keputusan pada \textit{k-member clustering} adalah \textit{NP-Hard}, artinya memiliki kompleksitas yang eksponensial.
\end{theorem}

\begin{proof}
Melalui pengamatan Aggarwal et al, permasalahan \textit{k-member clustering} dapat diselesaikan dengan kompleksitas polinomial.
\end{proof}

\begin{theorem}
N adalah total data dan k adalah parameter untuk anonimisasi. Setiap \textit{cluster} yang ditemukan oleh algoritma \textit{Greedy k-member clustering} memiliki jumlah tuple minimal sebanyak k, dan jumlah tuple tidak melebihi $2k - 1$.
\end{theorem}

\begin{proof}
S adalah himpunan data. Algoritma ini menemukan \textit{cluster} selama jumlah data yang tersisa sama dengan atau lebih besar dari k, setiap \textit{cluster} berisi k data. Jika total data pada S kurang dari k, maka sisa data akan dikelompokan pada  \textit{cluster} yang sudah ada. Oleh karena itu, ukuran maksimum sebuah \textit{cluster} adalah $2k - 1$.
\end{proof}

\begin{theorem}
N adalah jumlah data dan k menjadi parameter anonimisasi yang ditentukan. Jika $n > k$, kompleksitas algoritma \textit{Greedy k-member clustering} adalah $O(n^2)$.
\end{theorem}

\begin{proof}
Algoritma \textit{Greedy k-member clustering} menghabiskan sebagian besar waktunya untuk memilih data dari S satu per satu hingga mencapai $|S| = k$. Karena ukuran set input berkurang satu pada setiap iterasi, total waktu eksekusi adalah O $(n^2)$.
\end{proof}

\vspace{0.5cm}
\noindent Beberapa hal penting terkait algoritma \textit{Greedy k-means clustering}:

\begin{itemize}
\item Menetapkan tabel S  
\item Menetapkan nilai k
\item Menetapkan jumlah cluster (m) yang ingin dibuat
\begin{align}
m = \left \lfloor \frac{n}{k} \right \rfloor - 1
\end{align}
\end{itemize}

\vspace{0.5cm}
\noindent Berikut adalah langkah kerja algoritma \textit{Greedy k-means clustering} secara lengkap:

\begin{enumerate}
\item Melakukan inisialisasi variabel result dengan himpunan kosong dan variabel r dengan memilih data secara acak dari tabel S

\item Pada kondisi $|S| \geq k$, lakukan perulangan sebagai berikut:

\begin{enumerate}
\item Memilih data baru pada variabel r berdasarkan perbedaaan distance tertinggi dari nilai r sebelumnya. Perbedaan distance dapat dicari menggunakan rumus berikut:
\begin{align*}
\Delta (r_1,r_2) = \sum_{i=1}^{m} \delta_N(r_1[N_i],r_2	[N_i]) +  \sum_{j=1}^{n} \delta_C(r_1[C_j],r_2[C_j])
\end{align*}

\noindent Berikut adalah rumus menghitung distance antar data numerik:
\begin{align*}
\delta_n(v_1,v_2) = \frac{|v_1 - v_2|}{|D|} 
\end{align*}

\vspace{0.4cm}

\noindent Berikut adalah rumus menghitung distance antar data kategorikal:

\begin{align*}
\delta_C(v_1,v_2) = \frac{H(\Lambda(v_i,v_j))}{H(T_D)} 
\end{align*}

\vspace{0.4cm}

\item Membuang himpunan data variabel r pada variabel S

\item Mengisi data dari variabel r pada variabel c.

\item Pada kondisi $|c| \geq k$, lakukan perulangan sebagai berikut:

\begin{enumerate}
\item Memilih data baru terbaik untuk variabel r berdasarkan nilai \textit{Information Loss} (IL) yang paling rendah. \textit{Information Loss} (IL) dapat dicari menggunakan rumus berikut:

\begin{align*}
IL(e)&= |e| \cdot D(e) \\
D(e) &= \sum_{i=1}^{m} \frac{(MAX_{N_i} - MIN_{N_i})}{|N_i|} + \sum_{j=1}^{n}\frac{H(\Lambda(\cup_{C_j}))}{H(T_{C_j})}
\end{align*}

\item Membuang himpunan data dari variabel r pada variabel S

\item Menambahkan himpunan data dari variabel r pada variabel c.

\item Menambahkan himpunan data dari variabel c pada variabel result

\end{enumerate}

\end{enumerate}

\item Pada kondisi $|S| \neq  0$, artinya jika masih terdapat data yang belum dimasukkan pada sebuah \textit{cluster} dari tabel S, lakukan perulangan sebagai berikut:

\begin{enumerate}
\item Memilih data secara acak dari tabel S untuk disimpan pada variabel r

\item Membuang himpunan data dari variabel r pada variabel S

\item Memilih \textit{cluster} terbaik untuk variabel c berdasarkan nilai \textit{Information Loss} (IL) yang paling rendah. \textit{Information Loss} (IL) dapat dicari menggunakan rumus berikut:

\begin{align*}
IL(e)&= |e| \cdot D(e) \\
D(e) &= \sum_{i=1}^{m} \frac{(MAX_{N_i} - MIN_{N_i})}{|N_i|} + \sum_{j=1}^{n}\frac{H(\Lambda(\cup_{C_j}))}{H(T_{C_j})}
\end{align*}

\item Menambahkan himpunan data dari variabel r pada variabel c.

\end{enumerate}

\item Algoritma ini mengembalikan himpunan data berdasarkan jenis \textit{cluster} yang berbeda-beda melalui variabel \textit{result}.

\end{enumerate}

\noindent Berikut adalah pseudocode secara lengkap dari algoritma \textit{Greedy k-member clustering}:

\begin{algorithm}[H]
  \caption{Find Best Record}\label{alg:1}
  \begin{algorithmic}[1]
  %-------------- Input & Output -----------------
  \State \textbf{Function} \texttt{find\_best\_record(S,c)}
  \State \textbf{Input:} a set of records S and a cluster c.
  \State \textbf{Output:} a record r $\in$ S such that $IL(c \cup \{r\})$ is minimal
  \\
  %-------------- Baris 1-3 -----------------
  \State{$n = |S|$}
  \State{$min = \infty$}
  \State{$best = null$}
  \For{$i = 1 \ldots n$}
  \State{r = i-th record in S}
  \State{diff = $IL(c \cup \{r\}) - IL(c)$}
  \If{diff < min}
  \State{min = diff}
  \State{best = r}
  \EndIf
  \EndFor
  \State{return best}
  \end{algorithmic}
\end{algorithm}
Algoritma \ref{alg:1} menerima input himpunan data dataset dan sebuah data dengan nilai distance tertinggi dari data terpilih acak. Algoritma ini menghitung selisih \textit{distance} dari dua jenis data yang berbeda. Variabel \textit{diff} pada algoritma ini adalah perbedaan \textit{distance}, dicari dengan penjumlahan \textit{information loss} pada sebuah \textit{cluster} dengan \textit{information loss} pada data ke-i, lalu hasil penjumlahan tersebut dikurangi dengan \textit{information loss} dari \textit{kluster}. Output algoritma ini adalah sebuah data dengan nilai terbaik, yaitu data ke-i dari dataset S dengan nilai \textit{distance} paling kecil.
\begin{algorithm}[H]
  \caption{Find Best Cluster}\label{alg:2}
  \begin{algorithmic}[1]
  %-------------- Input & Output -----------------
  \State \textbf{Function} \texttt{find\_best\_cluster(C,r)}
  \State \textbf{Input:} a set of cluster C and a record r.
  \State \textbf{Output:} a cluster c $\in$ C such that $IL(c \cup \{r\})$ is minimal
  \\
  %-------------- Baris 1-3 -----------------
  \State{$n = |C|$}
  \State{$min = \infty$}
  \State{$best = null$}
  \For{$i = 1 \ldots n$}
  \State{c = i-th cluster in C}
  \State{diff = $IL(c \cup \{r\}) - IL(c)$}
  \If{diff < min}
  \State{min = diff}
  \State{best = c}
  \EndIf
  \EndFor
  \State{return best}
  \end{algorithmic}
\end{algorithm}

Algoritma \ref{alg:2} menerima input himpunan data \textit{cluster} dan sebuah data dengan nilai \textit{distance} tertinggi dari data terpilih acak. Algoritma ini menghitung selisih distance dari dua jenis data yang berbeda. Variabel \textit{diff} pada algoritma ini adalah perbedaan distance, dicari dengan penjumlahan \textit{information loss} pada sebuah \textit{cluster} dengan \textit{information loss} pada data ke-i, lalu hasil penjumlahan tersebut dikurangi dengan \textit{information loss} dari {cluster}. Output algoritma ini adalah data dengan nilai \textit{cluster} terbaik, yaitu data ke-i dari dataset S dengan nilai \textit{distance} paling kecil.

\begin{algorithm}[H]
  \caption{Greedy K-Member Clustering}		 \label{alg:3}
  \begin{algorithmic}[1]
  %-------------- Input & Output -----------------
  \State \textbf{Function} \texttt{greedy\_k\_member\_clustering(S,k)}
  \State \textbf{Input:} a set of records S and a threshold value k
  \State \textbf{Output:} a set of clusters each of which contains at least k records.
  \\
  %-------------- Baris 1-3 -----------------
  \If{$S \leq k$} 
  \State{return S}
  \EndIf
  \\
  \State{$result = \phi$}
  \State{r = a randomly picked record from S}
  \While{$|S| \geq k$}
  \State{r = the furthest record from r}
  \State{S = S - \{r\}}
  \State{c = \{r\}}
  	\While{$|c| < k$}
	\State{r = find\_best\_record(S,c)}  
	\State{S = S - \{r\}}
  	\State{c = c $\cup$ \{r\}}
  	\EndWhile
  	\State{result = result $\cup$ \{c\}}
  \EndWhile
  \While{$S \neq 0$}
  \State{r = a randomly picked record from S}
  \State{S = S - \{r\}}
  \State{c = find\_best\_cluster(result,r)}
  \State{c = c $\cup$ \{r\}}
  \EndWhile
  \State{return result}
  \end{algorithmic}
\end{algorithm}

Algoritma \ref{alg:3} menerima input himpunan data S dan nilai k. Algoritma ini mengeksekusi dua jenis fungsi yang berbeda yaitu fungsi \textit{find\_best\_cluster} untuk mencari \textit{cluster} dengan \textit{distance} terkecil dan fungsi \textit{find\_best\_record} untuk mencari data dengan \textit{distance} terkecil. Output dari algoritma ini adalah himpunan data dari berbagai jenis \textit{cluster} dengan nilai \textit{distance} terkecil.



\section{Metrik Distance, Information Loss} 
\label{theory:dist,il,cf}
Konsep PPDM memberikan solusi untuk mengukur tingkat keamanan, fungsionalitas, dan utilitas data menggunakan beberapa jenis metrik.  Beberapa metrik yang umum digunakan pada pengujian kualitas data yang telah dianonimisasi adalah \textit{distance} dan \textit{information loss}. Secara umum, pengukuran metrik dilakukan dengan membandingkan seberapa baik akurasi hasil data yang telah dianonimisasi dengan akurasi pada dataset sesungguhnya. 

\subsection{\textit{Distance}} 
\textit{Distance} adalah salah satu perhitungan untuk menyatakan akurasi terhadap utilitas sebuah data. \textit{Distance} merupakan faktor yang paling penting untuk menentukan hasil pengelompokan data. Pemilihan \textit{distance} yang baik dapat mencapai hasil klasifikasi dengan lebih optimal. Perhitungan \textit{distance} dilakukan berdasarkan pengelompokan tipe data numerik atau kategorikal. Karena masalah \textit{k-anonymity} menggunakan atribut numerik dan kategorikal, maka membutuhkan cara khusus untuk mengitung \textit{distance} dari kedua jenis data pada saat yang sama. 

\subsubsection{\textit{Distance} Data Numerik}
\textit{Distance} data numerik direpresentasikan sebagai nilai rentang. Beberapa atribut pada \textit{distance} numerik yaitu |D| adalah jumlah data pada sebuah domain berdasarkan satu atribut numerik, $v_1$, $v_2$ adalah nilai atribut numerik. \textit{Distance} data numerik dihitung menggunakan rumus berikut:

\begin{equation}
\delta_n(v_1,v_2) = \frac{|v_1 - v_2|}{|D|} 
\end{equation}

\subsubsection{Distance Data Kategorikal}
\textit{Distance} data kategorikal direpresentasikan sebagai \textit{taxonomy tree}. Beberapa atribut pada distance kategorikal yaitu |D| adalah jumlah data pada domain kategorikal, TD adalah \textit{taxonomy tree} untuk domain D,  $H(\Lambda(v_i,v_j))$ adalah jarak dari satu \textit{subtree} ke \textit{subtree} lain, $H(T_D)$ adalah tinggi dari \textit{taxonomy tree}. \textit{Distance} data kategorikal dihitung menggunakan rumus berikut:

\begin{equation}
\delta_C(v_1,v_2) = \frac{H(\Lambda(v_i,v_j))}{H(T_D)} 
\end{equation}

\subsubsection{Distance Record}
Beberapa atribut pada \textit{distance record} yaitu $r_1[N_i]$, $r_2[N_i]$ adalah nilai dari atribut numerik, $r_1[C_j]$, $r_2[C_j]$ adalah nilai dari atribut kategorikal, $\delta_N$ adalah \textit{distance} data numerik, $\delta_C$ adalah \textit{distance} data kategorikal. \textit{Distance} record dihitung menggunakan rumus berikut:
\begin{align}
\Delta (r_1,r_2) = \sum_{i=1}^{m} \delta_N(r_1[N_i],r_2	[N_i]) +  \sum_{j=1}^{n} \delta_C(r_1[C_j],r_2[C_j])
\end{align}


\subsection{Information Loss}
\textit{Information Loss} (IL) digunakan untuk mengevaluasi seberapa baik kinerja algoritma \textit{k-anonymity} terhadap utilitas sebuah data. Dalam menghitung \textit{Information Loss} (IL), perlu mendefinisikan beberapa atribut seperti \textit{cluster} $e = {r_1,\ldots,r_k}$  untuk \textit{quasi-identifier} yang terdiri dari atribut numerik ${N1,\ldots, Nm}$ dan atribut kategorikal ${C_1,\ldots,C_n}$, $T_{C_i}$ adalah \textit{taxonomy tree} untuk domain kategorikal $C_i$, $MIN_{N_i}$ dan $MAX_{N_i}$ adalah nilai minimum dan maksimum pada cluster $e$ untuk atribut Ni, $\cup_{C_i}$ adalah sekumpulan nilai pada \textit{cluster} $e$ berdasarkan atribut $C_i$. \\

\noindent \textit{Information loss} dihitung dengan rumus sebagai berikut:
\begin{align}
IL(e)&= |e| \cdot D(e) \\
D(e) &= \sum_{i=1}^{m} \frac{(MAX_{N_i} - MIN_{N_i})}{|N_i|} + \sum_{j=1}^{n}\frac{H(\Lambda(\cup_{C_j}))}{H(T_{C_j})}
\end{align}

\noindent Total \textit{Information Loss} dihitung dengan rumus sebagai berikut:
\begin{align}
Total-IL(AT) = \sum_{e \in \varepsilon}^{}  IL(e)
\end{align}

\noindent Semakin besar total \textit{information loss} yang dihasilkan maka informasi yang dihasilkan semakin kurang akurat. Oleh karena itu perlu dilakukan beberapa eksperimen terhadap penentuan nilai k pada algoritma \textit{Greedy k-member clustering} agar dihasilkan total \textit{information loss} seminimal mungkin sehingga hasil \textit{clustering} dan klasifikasi dengan nilai akurasi yang tinggi.

\section{Sistem Terdistribusi}
Sistem terdistribusi adalah kumpulan komputer berjalan secara independen dan saling terhubung dan saling bekerja sama untuk mencapai satu tujuan yang sama. Gambar   \ref{fig:sistemdistribusi1} adalah ilustrasi dari cara kerja sistem terdistribusi secara paralel.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.88]{sistemdistribusi1}
	\caption{Sistem Terdistribusi}
	\label{fig:sistemdistribusi1}
\end{figure}

\subsection{Manfaat Sistem Terdistribusi}
Sistem terdistribusi memiliki berbagai macam manfaat dalam melakukan komputasi. Berikut manfaat yang diperoleh dengan menggunakan sistem terdistribusi:

\begin{itemize}
\item \textit{Horizontal scalability}\\
Sistem terdistribusi menawarkan kemampuan untuk melakukan pemrosesan komputasi skala besar pada big data dengan harga yang murah.

\item \textit{Reliability}\\
Sistem terdistribusi dapat diandalkan karena proses komputasi pada sistem terdistribusi bergantung pada banyaknya komputer yang saling berkomunikasi satu sama lain untuk mencapai tujuan yang sama.
 
\item \textit{Performance}\\
Sistem terdistribusi dapat menangani proses komputasi tugas secara efisien karena beban kerja sesungguhnya dibagi menjadi beberapa bagian dan tersebar di beberapa komputer. 
\end{itemize}

\subsection{Tantangan Sistem Terdistribusi} 
Dalam perancangan sistem terdistribusi, sering ditemui kesulitan pada pemecahan masalah untuk kasus-kasus tertentu. Berikut adalah tantangan pada sistem terdistribusi:

\begin{itemize}
\item Penjadwalan\\
Kekuatan komputasi ada batasnya, sehingga sistem terdistribusi harus dapat memutuskan pekerjaan mana yang harus dikerjakan lebih dulu.
\item Latensi\\
Dengan pertukaran data antara perangkat keras dan perangkat lunak menggunakan jalur komunikasi jaringan, sehingga nilai latensi menjadi sangat tinggi. 
\item Observasi\\
Ketika sistem terdistribusi menjadi kompleks, kemampuan pengamatan untuk memahami kegagalan pada sistem terdistribusi merupakan tantangan besar komputer. 
\end{itemize}

\section{Big Data}
\textit{Big data} adalah data yang besar dan kompleks sehingga tidak mungkin sistem tradisional dapat memproses dan bekerja pada lingkungan data yang besar secara maksimal. Data dapat dikategorikan sebagai data besar berdasarkan berbagai faktor. Konsep utama yang umum dalam semua faktor  adalah jumlah data. Berikut adalah karakteristik 5V untuk \textit{big data}:

\begin{itemize}
\item \textit{Volume}\\
\textit{Volume} mengacu pada jumlah data yang sangat besar. Data tumbuh begitu besar sehingga sistem komputasi tradisional tidak lagi dapat menanganinya seperti yang kita inginkan.

\item \textit{Velocity}\\
\textit{Velocity} mengacu pada kecepatan di mana data dihasilkan. Setiap hari, sejumlah besar data dihasilkan, disimpan, dan dianalisis. Data dihasilkan dengan kecepatan kilat di seluruh dunia. Teknologi \textit{big data} memungkinkan untuk mengeksplorasi data, saat data itu dihasilkan.

\item \textit{Variety}\\
\textit{Variety} mengacu pada berbagai jenis data. Data terutama dikategorikan ke dalam data terstruktur dan tidak terstruktur. Faktanya, lebih dari 75 persen data dunia ada dalam bentuk yang tidak terstruktur. 

\item \textit{Veracity}\\
\textit{Veracity} mengacu pada kualitas data. Ketika menyimpan beberapa data yang besar, apabila tidak ada gunanya di masa depan, maka membuang-buang sumber daya untuk menyimpan data tersebut. Jadi, kita harus memeriksa kepercayaan data sebelum menyimpannya. 

\item \textit{Value}\\
\textit{Value} adalah bagian terpenting dari big data. Organisasi menggunakan data besar untuk menemukan nilai informasi baru. Menyimpan sejumlah besar data sampai pada ekstraksi informasi yang bermakna dari sekumpulan data tersebut.
\end{itemize}

\noindent Analisis \textit{big data} adalah proses menggunakan algoritma analisis yang berjalan pada \textit{platform} pendukung yang kuat untuk mengungkap potensi yang disembunyikan dalam data besar, seperti pola tersembunyi atau korelasi yang tidak diketahui. Analisis \textit{big data} dapat dikategorikan ke dalam dua jenis pemrosesan. Berikut adalah jenis analisis pada \textit{big data}:

\begin{itemize}
\item 
\textit{Batch Processing}\\
\textit{Batch processing} adalah sekumpulan data disimpan terlebih dahulu, lalu pada waktu tertentu data yang telah terkumpul akan dilakukan analisis. \textit{MapReduce} telah menjadi model penerapan batch processing pada umumnya. Ide dari \textit{MapReduce} adalah data yang besar dibagi menjadi potongan data yang lebih kecil. Selanjutnya, potongan-potongan data ini diproses secara paralel dan secara terdistribusi untuk dianalisis lebih lanjut. 

\item
\textit{Streaming Processing}\\
\textit{Streaming processing} mengasumsikan nilai informasi data yang diperoleh bergantung kepada seberapa cepat data dapat diolah secara \textit{real time}. Pengertian dari \textit{real time} adalah data diolah secara langsung ketika data tersebut diperoleh. Data diambil melalui aliran data. Karena aliran data yang masuk sangat cepat dan membawa volume yang cukup besar, maka hanya sebagian kecil data yang dapat disimpan dan diolah dalam memori yang terbatas. 
\end{itemize}

\noindent Diperlukan teknologi tertentu untuk melakukan komputasi pada lingkungan  \textit{big data}. Pada bagian selanjutnya akan dijelaskan konsep-konsep terkait penggunaan \textit{framework} beserta komponen-komponen penting pada \textit{framework} tersebut terkait komputasi pada lingkungan \textit{big data}. \textit{Framework} tersebut antara lain Hadoop dan Spark. Masing-masing \textit{framework} akan diteliti lebih lanjut, untuk dipilih pada penelitian ini berdasarkan kecepatan waktu komputasi.

\section{Hadoop}
Hadoop adalah \textit{framework} yang memanfaatkan beberapa komputer untuk menyelesaikan masalah yang melibatkan volume data sangat besar. Hadoop memecah input dari pengguna menjadi beberapa blok data dan masing-masing blok data diproses menggunakan konsep \textit{MapReduce} di mana data akan diproses secara paralel pada sistem terdistribusi. 

\subsection{HDFS}
HDFS adalah sistem \textit{file} terdistribusi pada Hadoop dengan menyediakan penyimpanan data yang handal, mendukung partisi, dan toleran terhadap kesalahan pada hardware. HDFS bekerja erat dengan MapReduce dengan mendistribusikan penyimpanan dan perhitungan di seluruh \textit{cluster} dengan menggabungkan sumber daya penyimpanan yang dapat dipartisi tergantung kebutuhan. 

\subsection{MapReduce}
\textit{MapReduce} adalah kerangka kerja pada komputasi terdistribusi dengan membagi pekerjaan besar yang kompleks menjadi beberapa pekerjaan kecil dan dilakukan secara paralel. 

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.32]{MapReduceImage}
	\caption{Tahapan pada MapReduce}
	\label{fig:MapReduceImage}
\end{figure}

\noindent Gambar \ref{fig:MapReduceImage} adalah tahapan pada \textit{MapReduce}, berikut adalah penjelasannya:

\begin{itemize}
\item Input\\
Input yang diterima adalah blok-blok data yang telah tersebar ke seluruh \textit{cluster}.

\item \textit{Mapper}\\
\textit{Mapper} memiliki fungsi untuk memetakan blok data kedalam pasangan <\textit{key},\textit{value}>. 

\item Reducer\\
Berikut adalah tahapan yang dilakukan oleh \textit{Reducer}:

\begin{enumerate}

\item Shuffle \\
\textit{Shuffle} adalah fase pada data antara untuk menggabungkan semua nilai menjadi koleksi yang terkait dengan kunci yang sama.


\item \textit{Sort} \\
Pasangan <\textit{key},\textit{value}> pada satu node secara otomatis diurutkan oleh Hadoop sebelum diberikan kepada \textit{reducer}. Penyortiran dilakukan berdasarkan keterurutan nilai \textit{key}. 


\item \textit{Reducer} \\
Hasil \textit{mapper} yang diacak dan diurutkan disediakan untuk \textit{Reducer}. Tahap ini membuat pasangan <\textit{key}, \textit{(list of value)}> baru berdasarkan pengelompokan \textit{key}.
\end{enumerate}

\item Output\\
Output yang dihasilkan adalah pasangan <\textit{key}, \textit{(list of value)}> baru berdasarkan jenis komputasi tertentu. Output ini akan ditulis ke file oleh Reducer.
\end{itemize}

\section{Spark} 
\label{sec:konsep_spark}
Spark adalah teknologi komputasi \textit{cluster} yang dirancang untuk komputasi cepat. Spark adalah paradigma pemrosesan data berukuran besar yang dikembangkan oleh para peneliti University of California di Berkeley. Spark adalah alternatif dari Hadoop MapReduce untuk mengatasi keterbatasan pemrosesan input output yang tidak efisien pada disk, dengan menggunakan memori. Fitur utama Spark adalah melakukan komputasi di dalam memori sehingga waktu komputasi menjadi lebih singkat dibandingkan waktu komputasi di dalam disk.
\\\\
Berikut adalah karakteristik dari Spark:
\begin{itemize}
\item Kecepatan\\
Spark adalah alat komputasi klaster tujuan umum. Ini menjalankan aplikasi hingga 100 kali lebih cepat dalam memori dan 10 kali lebih cepat pada disk daripada Hadoop. Spark mengurangi jumlah operasi baca/tulis pada disk dan menyimpan data dalam memori.

\item Mudah untuk diatur\\	
Spark dapat melakukan pemrosesan batch, analisis data secara interaktif, machine learning, dan streaming data. Semuanya pemrosesan tersebut dikerjakan pada satu komputer yang sama. Fungsi ini menjadikan Apache Spark sebagai mesin analisis data yang lengkap. 


\item Analisis secara real-time\\
Spark dapat dengan mudah memproses data \textit{real-time}, misalnya \textit{streaming} data secara \textit{real-time} untuk ribuan peristiwa/detik. Contoh dari sumber \textit{streaming} data adalah Twitter, Facebook, Instagram. \textit{Streaming} data dapat diproses secara efisien oleh Spark.
\end{itemize}

\subsection{Ekosistem Spark}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.18]{spark_ecosystem}
	\caption{Ekosistem Spark}
	\label{fig:spark_ecosystem}
\end{figure}
Gambar \ref{fig:spark_ecosystem} menunjukan bahwa Spark bekerja sama dengan teknologi \textit{big data} lain untuk memenuhi berbagai macam kebutuhan dalam pengolahan \textit{big data}. Masing-masing warna pada Gambar \ref{fig:spark_ecosystem} mewakili jenis teknologi yang dipakai pada Spark. Spark SQL, Spark Streaming, Spark MLlib adalah library tambahan pada Spark. Cassandra, Kafka, dan ElasticSearch adalah \textit{framework} untuk melakukan pengumpulan data secara \textit{streaming}. Sedangkan scala, java, dan python adalah bahasa pemrograman yang dapat digunakan pada Spark.

\subsection{Arsitektur Spark}
\label{sec:arsitektur_spark}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{arsitektur_spark2}
	\caption{Arsitektur Spark}
	\label{fig:arsitektur_spark2}
\end{figure}
Berdasarkan Gambar \ref{fig:arsitektur_spark2}, berikut adalah tahapan kerja pada arsitektur Spark:

\begin{enumerate}

\item
\textit{Program Driver} memanggil program utama aplikasi dan membuat \textit{SparkContext} dan \textit{Spark Driver}. \textit{SparkContext} terdiri dari semua fungsi dasar. \textit{Spark Driver} berisi \textit{DAG Scheduler, Task Scheduler, Backend Scheduler}, dan \textit{Block Manager}.

\item
\textit{Spark Driver} dan \textit{SparkContext} secara kolektif mengawasi pelaksanaan pekerjaan di dalam \textit{cluster}. \textit{Spark Driver} bekerja sama dengan \textit{Cluster Manager} untuk membagian pekerjaan ke setiap \textit{Worker Node}. 

\item
\textit{Worker Node} menjalankan tugas yang diberikan oleh \textit{Cluster Manager} dan mengembalikannya ke \textit{SparkContext}. \textit{Worker Node} bertanggung jawab atas pelaksanaan tugas yang diberikan. 

\end{enumerate}


\subsection{Jenis Instalasi pada Spark}
\label{sec:instalasi_spark}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{arsitektur_spark}
	\caption{Arsitektur Spark}
	\label{fig:arsitektur_spark}
\end{figure}

Berdasarkan Gambar \ref{fig:arsitektur_spark}, berikut adalah jenis-jenis instalasi pada Spark:

\begin{itemize}
\item \textit{Standalone}\\  
Spark berdiri diatas HDFS Hadoop. Spark memungkinkan untuk mengakses data pada HDFS Hadoop untuk membaca input dan menulis output.

\item \textit{Hadoop Yarn}\\
Spark dapat berjalan pada Hadoop Yarn tanpa memerlukan instalasi atau meminta hak akses root apapun. Hadoop Yarn membantu integrasi Spark pada ekosistem Hadoop.

\item \textit{Spark In MapReduce} (SIMR)\\ 
SIMR digunakan untuk menjalankan pekerjaan Spark secara independen. Jenis instalasi ini sudah tidak lagi berlaku untuk Spark versi 2.0
\end{itemize}

\subsection{Resilient Distibuted Datasets (RDD)}
\label{sec:rdd}
\par RDD adalah kumpulan partisi terdistribusi yang disimpan dalam memori atau \textit{disk} pada beberapa \textit{cluster}. RDD tersebar menjadi beberapa partisi,  sehingga partisi tersebut dapat disimpan dan diproses pada komputer yang berbeda. 

\subsubsection{Karakteristik pada RDD}
\noindent Berikut adalah beberapa sifat pada RDD dan berikut penjelasannya:
\begin{itemize}
\item \textit{Lazy evaluation}: operasi pada Spark hanya akan dilakukan ketika memanggil fungsi \textit{Action}.

\item \textit{Immutability}: data yang disimpan dalam RDD tidak dapat diubah nilainya. 

\item \textit{Fault tolerance}: RDD dapat melakukan toleransi kesalahan pada \textit{hardware}.

\item \textit{In-memory computation}: RDD menyimpan data secara langsung dalam memori.

\item \textit{Partitioning}: membagi perkerjaan pada beberapa komputer.
\end{itemize}

\subsubsection{Fungsi Komputasi pada RDD}
\label{sec:fungsi_rdd}
\noindent Berikut adalah penjelasan singkat dan contoh dari jenis operasi pada RDD:

\begin{itemize}
\item Fungsi \textit{Transformation}\\
Fungsi \textit{transformation} dilakukan secara \textit{lazy}, sehingga hanya akan dikerjakan apabila dipanggil pada fungsi \textit{action}. Fungsi \textit{transformation} pada RDD akan dijelaskan pada tabel dibawah ini.\\

\begin{tabular}{|l|p{10cm}|}
\hline 
\rule[-1ex]{0pt}{2.5ex} Fungsi & Deskripsi \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} map() & Mengembalikan RDD baru dengan menerapkan fungsi pada setiap elemen data \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} filter() & Mengembalikan RDD baru yang dibentuk dengan memilih elemen-elemen sumber di mana fungsi mengembalikan true \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} reduceByKey() & Menggabungkan nilai-nilai kunci menggunakan fungsi \\ 
\hline 
\end{tabular} 

\vspace{0.2cm}

\item Fungsi \textit{Action}\\
Fungsi \textit{Action} adalah operasi yang mengembalikan nilai output ke dalam terminal atau melakukan penulisan data pada sistem penyimpanan eksternal. Fungsi \textit{Action} memaksa evaluasi pada RDD yang akan dipanggil, untuk menghasilkan output. Fungsi \textit{Action} pada RDD akan dijelaskan pada tabel dibawah ini.\\

\begin{tabular}{|l|p{10cm}|}
\hline 
\rule[-1ex]{0pt}{2.5ex} Fungsi & Deskripsi \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} count() & Mendapat jumlah elemen data dalam RDD \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} reduce() & Agregat elemen data ke dalam RDD dengan mengambil dua argumen dan mengembalikan satu \\ 
\hline 
\rule[-1ex]{0pt}{2.5ex} foreach(operation) & Menjalankan operasi untuk setiap elemen data dalam RDD \\ 
\hline 
\end{tabular} 
\end{itemize}

\subsection{Dataframe}
\label{sec:dataframe}
\textit{Dataframe} adalah kumpulan data yang didistribusikan, disusun dalam baris dan kolom. Setiap kolom dalam \textit{Dataframe} memiliki nama dan tipe terkait. \textit{Dataframe} mirip dengan tabel database tradisional, yang terstruktur dan ringkas.Dengan menggunakan \textit{Dataframe}, kueri SQL dapat dengan mudah diimplementasi pada \textit{big data}.

\subsection{Komponen Spark}
\label{sec:komponen_spark}
Komponen Spark adalah library tambahan pada Spark untuk melakukan proses komputasi pada lingkungan big data berdasarkan jenis-jenis kebutuhan pengolahan data. Berikut adalah penjelasan singkat mengenai komponen pada Spark:

\begin{itemize}
\item Spark Core \\
Spark Core adalah \textit{library} Spark yang berisi fungsionalitas dasar Spark, termasuk komponen untuk penjadwalan tugas, manajemen memori, pemulihan kesalahan, dan berinteraksi dengan sistem penyimpanan. Spark Core menyediakan komputasi pada memori, fungsi \textit{action} dan \textit{transformation} untuk mengolah RDD.

\item Spark SQL  \\
Spark SQL memungkinkan pemrosesan kueri SQL pada lingkungan big data. Spark SQl menyediakan fungsi untuk menghitung nilai statistik dasar seperti \textit{mean, median, modus}, nilai maksimum dan nilai minimum.

\item Spark Streaming \\
Spark Streaming adalah salah satu komponen Apache Spark, yang memungkinkan Spark dapat memproses data \textit{streaming} secara \textit{real-time}. Spark Streaming menyediakan API untuk memanipulasi aliran data yang cocok dengan RDD. Hal ini memungkinkan analisis data untuk beralih melalui sumber aplikasi yang memberikan data secara \textit{real-time}. 

\item
Spark MLlib \\
Spark MLlib adalah \textit{library} Spark yang berisi fungsionalitas yang umum digunakan pada \textit{machine learning}. Untuk mengimplementasikan teknik \textit{data mining} pada lingkungan \textit{big data} dibutuhkan \textit{library} Spark MLlib. Spark MLlib menyediakan berbagai jenis algoritma \textit{machine learning} termasuk klasifikasi dan pengelompokan/\textit{clustering}.
\end{itemize}

\section{Spark MLlib}
\label{sec:konsep_spark_mllib}
Spark MLlib adalah library pembelajaran mesin berdasarkan komputasi secara paralel. MLlib terdiri dari algoritma pembelajaran umum seperti klasifikasi, pengelompokan/\textit{clustering}. Secara garis besar, MLlib melakukan data \textit{preprocessing}, pelatihan model, dan membuat prediksi.

\subsection{Machine Learning pada Spark MLlib}
\label{sec:ml_sparkmllib}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{machinelearningmllib}
	\caption{Tahapan Pembelajaran Machine Learning}
	\label{fig:machinelearningmllib}
\end{figure}

\textit{Machine learning} bertujuan membuat prediksi label/kelompok data berdasarkan jenis model yang dipakai. Pemodelan \textit{machine learning} mencakup model dari\textit{data mining}. Pemodelan \textit{machine learning} membutuhkan input berupa vektor fitur. Vektor fitur adalah nilai masing-masing atribut yang digunakan pada pelatihan data. 

\noindent Gambar \ref{fig:machinelearningmllib} adalah tahapan \textit{machine learnin}g pada Spark MLlib, berikut adalah penjelasan singkat dari masing-masing tahapan:

\begin{enumerate}

\item \textit{Featurization}\\
Pemodelan machine learning hanya dapat menerima input berupa vektor. Oleh karena itu, nilai atribut pada tabel akan diubah ke representasi numerik dalam bentuk vektor. 
 
\item \textit{Training}\\
Pemodelan machine learning melakukan pelatihan agar model yang dipakai memberikan hasil yang tepat untuk menentukan label atau kelompok data. Oleh karena itu, pemodelan machine memerlukan pelatihan model beberapa kali untuk mendapatkan model terbaik.

\item \textit{Model Evaluation}\\
Pada akhir pelatihan, model yang terbentuk dapat diputuskan baik atau tidak melalui perhitungan nilai akurasi. Semakin besar nilai akurasi, maka model dapat digunakan untuk memprediksi nilai label atau kelompok data secara tepat.
\end{enumerate}

\subsection{Tipe Data pada Spark MLlib}
Seperti yang sudah dijelaskan pada bagian \ref{sec:ml_sparkmllib}, pemodelan \textit{machine learning} menerima input berupa vektor fitur. Tipe data yang disediakan pada Spark MLlib terdiri dari beberapa jenis yaitu vektor, \textit{labeledpoint}, dan \textit{various model class}. \\

\noindent Berikut adalah penjelasan jenis tipe data pada Spark MLlib:

\begin{itemize}
\item Vektor\\
Vektor terdiri dari dua jenis yaitu vektor dense dan vektor sparse. Kelas vektor berada pada \textit{package} mllib.linalg.Vectors. Gambar \ref{fig:vektor} adalah contoh vektor dense dan vektor sparse:

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{vektor}
	\caption{Contoh Vektor Dense dan Sparse}
	\label{fig:vektor}
\end{figure}

\begin{itemize}

\item Vektor dense\\
Vektor dense adalah vektor yang menyimpan setiap nilai fitur dataset. Jumlah elemen pada vektor dense akan memiliki jumlah yang sama dengan jumlah fitur pada dataset.

\item Vektor sparse\\
Vektor sparse adalah vektor yang menyimpan setiap nilai fitur yang bukan nol pada dataset, sehingga jumlah elemen yang disimpan pada vektor sparse lebih sedikit dibandingkan dengan jumlah elemen yang disimpan pada vektor dense. 

\end{itemize}

\item \textit{LabeledPoint}\\
\textit{LabeledPoint} digunakan pada algoritma \textit{supervised learning} yaitu klasifikasi dan regresi. Kelas \textit{LabeledPoint} terletak pada \textit{package} mllib.regress.

\item \textit{Various Model class}\\
\textit{Various Model classes} adalah tipe data yang dihasilkan dari pemodelan \textit{machine learning}. Tipe data ini memiliki fungsi predict() untuk melakukan prediksi label dan kelompok data.

\end{itemize}

\subsection{Data Mining pada Spark MLlib}
Data mining pada Spark MLlib menggunakan tahapan pemodelan pada \textit{machine learning} yang dijelaskan pada bagian \ref{sec:ml_sparkmllib} untuk menghasilkan tabel hasil pengelompokan dan klasifikasi. Pada bagian ini akan dijelaskan parameter dari pemodelan Spark MLlib.

\subsubsection{Naive Bayes}
\label{sec:naivebayes_mllib}
\textit{Naive Bayes} menjadi pemodelan klasifikasi yang umum digunakan. \textit{Naive Bayes} dapat dilatih dengan sangat efisien karena prosesnya hanya menghitung probabilitas bersyarat. \textit{Naive Bayes} memiliki parameter masukan sebagai berikut:
 
\begin{itemize}
\item \textit{randomSplit} adalah membagi training dan test data berdasarkan persentase.
\item \textit{setModelType} adalah memilih model yang tersedia (\textit{multinomial/bernoulli})
\item \textit{setLabelCol} adalah memilih jenis atribut yang menjadi label kelas.
\end{itemize}

\subsubsection{K-Means}
\label{sec:kmeans_mllib}
\textit{K-means} menjadi pemodelan pengelompokan/\textit{clustering} yang paling umum digunakan untuk mengelompokkan titik-titik data menjadi sejumlah kelompok yang telah ditentukan. \textit{K-means} memiliki parameter masukan sebagai berikut:

\begin{itemize}
\item \textit{k} adalah jumlah cluster yang diinginkan. 
\item \textit{maxIterations} adalah jumlah iterasi maksimum yang harus dijalankan.
\item \textit{initializationMode} menentukan inisialisasi centroid secara acak.
\item \textit{initializationSteps} menentukan jumlah langkah dalam algoritma \textit{k-means}.
\item \textit{initialModel} adalah menentukan nilai centroid saat inisialisasi.
\end{itemize}

\section{Scala}
\label{sec:scala}
Scala adalah bahasa pemrograman berbasi open source, dibuat oleh Profesor Martin Odersky. Scala adalah bahasa pemrograman multi-paradigma dan mendukung paradigma fungsional serta berorientasi objek. Untuk pengembangan Spark, penulisan sintaks Scala dianggap produktif untuk mengimplementasikan kode program. Pemrograman pada Scala mempertahankan prinsip keseimbangan antara produktivitas pengembangan program dan kinerja program. Pemrograman pada Scala tidak serumit pemrograman pada Java. Satu baris kode program pada Scala dapat menggantikan 20 hingga 25 baris kode Java. Karena alasan terbut, Scala menjadi bahasa pemrograman yang sangat diminati untuk melakukan pemrosesan \textit{big data} pada Spark.

\section{Scala Swing} 
Scala Swing adalah program berbasis \textit{Graphical User Interface} (GUI) sehingga memiliki perbedaan dengan program Spark yang dieksekusi dengan terminal. Scala Swing bertujuan untuk memberi tampilan program sehingga hasil program diharapkan menjadi lebih interaktif. Scala menyediakan akses langsung terhadap kelas GUI pada Java menggunakan \textit{library} Scala Swing.  Dengan menggunakan Scala, penggunaan Scala Swing dapat memenuhi kebutuhan perancangan \textit{User Interface} melalui berbagai macam komponen GUI pada umumnya. Gambar \ref{fig:swing_example} adalah contoh implementasi GUI sederhana pada Scala Swing.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.9]{swing_example}
	\caption{GUI Sederhana pada Scala Swing}
	\label{fig:swing_example}
\end{figure}


\subsection{Panel dan Layout}
\textit{Panel} adalah tempat untuk menampilkan semua komponen GUI dengan beberapa aturan tata letak yang harus dipenuhi. Salah satu bagian tersulit pada perancangan aplikasi berbasis GUI adalah mengatur penempatan layout dengan benar. \textit{Layout} terdiri dari beberapa komponen GUI seperti \textit{Frame, Panel, Label} atau \textit{Button}. Masing-masing komponen GUI pada \textit{layout} memiliki nilai properti sendiri (warna, ukuran, posisi) yang dapat diatur secara manual.

\subsection{Handling Event}
\textit{Handling event} adalah perkerjaan yang dilakukan masing-masing komponen. Komponen akan menerima aksi langsung dari pengguna aplikasi. Mekanisme ini dikenal sebagai \textit{handling event}, yang dieksekusi ketika suatu peristiwa terjadi. \textit{Handling event} memiliki \textit{listener}. \textit{Listener} adalah sebuah komponen memberi tahu sebuah aksi kepada komponen tertentu. \textit{Listener} harus dibuat untuk masing-masing objek \textit{handling event}. 

\section{Format CSV}
\label{theory:csv}
\textit{Comma Separated Values} (CSV) menjadi format yang sangat umum digunakan untuk menyimpan nilai pada tabel data. CSV menyimpan data untuk setiap baris data yang nilainya dipisahkan oleh tanda koma. Gambar \ref{fig:csv} adalah contoh dari format penyimpanan CSV.
  
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{csv}
	\caption{Format CSV}
	\label{fig:csv}
\end{figure}

\noindent Berikut adalah contoh penggunaan format CSV pada Spark:

\begin{itemize}

\item Mengambil \textit{file} CSV \\
Cara ini dipakai untuk mengambil data input dalam format CSV pada sebuah direktori. Fungsi untuk mengambil file CSV pada Spark adalah \textsf{textFile()}. 

\item Menyimpan \textit{file} CSV \\
Cara ini dipakai untuk menyimpan hasil pengolahan data dalam format CSV pada sebuah direktori. Fungsi untuk menyimpan hasil pengolahan data dalam file CSV pada Spark adalah \textsf{write.csv()}. Fungsi ini dipanggil apabila mengembalikan output dalam bentuk \textit{file}.

\end{itemize}


